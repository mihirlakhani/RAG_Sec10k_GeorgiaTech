
# Project Overview

This project applies **Generative AI (Gen AI)** through a **Retrieval Augmented Generation (RAG)** pipeline to extract insights from **SEC 10-K filings**. The 10-K filings are annual reports by publicly traded companies that detail their financial health, business operations, and risks.

We concentrate on Item 7 (Management's Discussion and Analysis - MD&A) and Item 8 (Financial Statements and Supplementary Data), which hold key insights into a company's strategy, performance, and risk profile.

## Table of Contents
1. [Tech Stack & Rationale](#tech-stack)
2. [Installation and Setup](#installation-and-setup)
3. [Usage](#usage)
4. [Demo Video](#demo-video)
5. [Future Enhancements](#future-enhancements)
6. [Credits and References](#credits-and-references)


## Tech Stack
The technology stack used in this project was carefully selected to construct a Retrieval Augmented Generation (RAG) pipeline for analyzing SEC 10-K filings. Below is a detailed explanation of what RAG is, why it was chosen, and how it benefits this particular project.

### Understanding Retrieval Augmented Generation (RAG)
- **Definition**: RAG is a technique in NLP that combines retrieval-based systems with generation-based models. It allows a large language model (LLM) to retrieve relevant context from a designated source, such as a database, index, or document corpus, before generating a response or performing further analysis.
- **Workflow**: In a typical RAG pipeline, the retrieval process identifies and fetches relevant information based on a query or context. This information is then fed into a generation model, such as Google's PaLM, which uses the retrieved context to produce a response, generate text, or derive insights.

### Why RAG was Chosen
- **Focused Context**: Given the large volume of information in SEC 10-K filings, RAG is advantageous because it allows the LLM to focus on specific segments of the filings that are most relevant to a given query. This targeted approach reduces the need to process entire documents and improves accuracy.
- **Scalability**: RAG is scalable because it separates the retrieval process from the generation process. This allows for easy scaling as new data or documents are added, without the need to retrain the underlying LLM.
- **Reduced Training Requirements**: Unlike fine-tuning, which requires retraining a model with new data, RAG relies on pre-trained LLMs and supplements them with additional context through retrieval. This reduces the computational overhead and time needed to adjust a model to new data sources.
- **Flexibility**: RAG allows for flexibility in choosing the source of context and the generation model. In this project, LlamaIndex is used to create an index over SEC 10-K filings, providing a structured retrieval system. This flexibility allows you to switch retrieval sources or generation models without extensive reconfiguration.

### Benefits over Fine-Tuning
- **Cost-Effectiveness**: Fine-tuning a large language model can be computationally expensive and time-consuming. RAG avoids this by using pre-trained models and focusing on retrieval for contextualization.
- **Reduced Data Dependencies**: Fine-tuning often requires significant amounts of domain-specific data, while RAG relies on existing datasets or documents, reducing the need for large-scale data collection and processing.
- **Dynamic Adaptability**: RAG allows for dynamic adaptation to new data sources without retraining the LLM. This adaptability is crucial in scenarios like SEC 10-K filings, where the content can change annually or with each new report.
- **Enhanced Interpretability**: By retrieving specific segments from the document corpus, RAG enhances interpretability, making it easier to understand which parts of the source material were used to generate insights or responses.

### RAG Pipeline Architecture
The diagram below illustrates the Retrieval Augmented Generation (RAG) architecture for this project, showing the key components and their relationships.

![RAG Architecture](docs/rag_architecture.png)


### Specific Tech Stack Components
- **LlamaIndex**: Used to orchestrate the RAG pipeline by creating and managing indexes over SEC 10-K filings. It connects the retrieval and generation components.
- **BAAI/bge-small-en-v1.5 (Hugging Face)**: Provides embeddings for the text, allowing for semantic similarity and efficient retrieval.
- **Google PaLM**: A pre-trained LLM used for generating insights based on the retrieved context.
- **sec-api.io/extractor**: Facilitates extracting specific sections from SEC 10-K filings, enabling focused analysis.
- **Flask**: Serves as the web application framework for building a simple user interface to display insights generated by the RAG pipeline.
- **sec-edgar-downloader**: Allows automated downloading of SEC 10-K filings from the EDGAR database, providing the raw data for indexing and retrieval.

Each component plays a critical role in the overall RAG pipeline, providing a cohesive solution for efficient retrieval, contextualization, and text generation, without the need for extensive fine-tuning or retraining of models.

## Installation and Setup
To set up this project on your local environment:
1. Clone the repository:
   ```bash
   git clone <repository-url>
   ```
2. Ensure you have Python and the necessary dependencies installed:
   ```bash
   pip install -r requirements.txt
   ```
3. Configure your environment to access PaLM (set API keys or other credentials as required).
4. Set any necessary environment variables or configuration files for the Flask app.

## Usage
To run the Flask web application:
1. Start the Flask server:
   ```bash
   python app.py
   ```
2. Open the web application in a browser using the provided URL or local host.


## Demo Video

- [https://drive.google.com/file/d/1HWe3S0dMUXG7rM8MmhrbPu8me125TwxI/view?usp=drive_link]


## Future Enhancements
Potential areas for future work include:
- Enhancing the RAG pipeline by adding a vectordb like pinecone or chromdb.
- Adding more complex data retrieval techniqes or query embedding match techniques
- Integrating with other data sources for a broader financial analysis.

## Credits and References
This project was developed for the Financial Services Innovation Lab at Georgia Tech. The following tools and frameworks were used:
- LlamaIndex for orchestrating the RAG pipeline.
- Google's PaLM for text analysis.
- HuggingFace for Embedding Model
- sec-api.io/extractor for data extraction
- Flask for building the web application.
- sec-edgar-downloader for retrieving SEC filings.

---

.
